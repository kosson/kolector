# Resurse

## The Programming Historian

https://programminghistorian.org/

We publish novice-friendly, peer-reviewed tutorials that help humanists learn a wide range of digital tools, techniques, and workflows to facilitate research and teaching. We are committed to fostering a diverse and inclusive community of editors, writers, and readers.

https://programminghistorian.org/en/lessons/cleaning-data-with-openrefine

### OCR and Machine Translation

https://programminghistorian.org/en/lessons/OCR-and-Machine-Translation

This lesson covers how to convert images of text into text files and translate those text files. The lesson will also cover how to organize and edit images to make the conversion and translation of whole folders of text files easier and more accurate. The lesson concludes with a discussion of the shortcomings of automated translation and how to overcome them.

### Working with batches of PDF files

https://programminghistorian.org/en/lessons/working-with-batches-of-pdf-files

Learn how to perform OCR and text extraction with free command line tools like Tesseract and Poppler and how to get an overview of large numbers of PDF documents using topic modeling.

### Introduction to Populating a Website with API Data

https://programminghistorian.org/en/lessons/introduction-to-populating-a-website-with-api-data

This lesson introduces a way to populate a website with data obtained from another website via an Application Programming Interface (API). Using some simple programming, it provides strategies for customizing the presentation of that data, providing flexible and generalizable skills.

### Analyzing Documents with TF-IDF

https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf

This lesson focuses on a foundational natural language processing and information retrieval method called Term Frequency - Inverse Document Frequency (tf-idf). This lesson explores the foundations of tf-idf, and will also introduce you to some of the questions and concepts of computationally oriented text analysis.

### Using Geospatial Data to Inform Historical Research in R

https://programminghistorian.org/en/lessons/geospatial-data-analysis#visualizing

In this lesson, you will use R-language to analyze and map geospatial data.

## Cărți R

### Hands-On Programming with R

Garrett Grolemund

https://jjallaire.github.io/hopr/

This is the website for **“Hands-On Programming with R”**. This book will teach you how to program in R, with hands-on examples. I wrote it for non-programmers to provide a friendly introduction to the R language. You’ll learn how to load data, assemble and disassemble data objects, navigate R’s environment system, write your own functions, and use all of R’s programming tools. Throughout the book, you’ll use your newfound skills to solve practical data science problems.

### Welcome to Text Mining with R

https://www.tidytextmining.com/index.html

1.4 The gutenbergr package
Now that we’ve used the janeaustenr package to explore tidying text, let’s introduce the gutenbergr package (Robinson 2016). The gutenbergr package provides access to the public domain works from the Project Gutenberg collection. The package includes tools both for downloading books (stripping out the unhelpful header/footer information), and a complete dataset of Project Gutenberg metadata that can be used to find works of interest. In this book, we will mostly use the function gutenberg_download() that downloads one or more works from Project Gutenberg by ID, but you can also use other functions to explore metadata, pair Gutenberg ID with title, author, language, etc., or gather information about authors.

### ggplot2. Elegant graphics for Data Analysis

https://ggplot2-book.org/

This is the on-line version of work-in-progress **3rd edition** of “ggplot2: elegant graphics for data analysis” published by Springer. You can learn what’s changed from the 2nd edition in the [Preface](https://ggplot2-book.org/preface-3e.html#preface-3e).

While this book gives some details on the basics of ggplot2, its primary focus is explaining the Grammar of Graphics that ggplot2 uses, and describing the full details. It is not a [cookbook](https://r-graphics.org/), and won’t necessarily help you create any specific graphic that you need. But it will help you understand the details of the underlying theory, giving you the power to tailor any plot specifically to your needs.

The book is written by [Hadley Wickham](http://hadley.nz/), [Danielle Navarro](https://djnavarro.net/), and [Thomas Lin Pedersen](https://www.data-imaginist.com/).

### R Graphics Cookbook, 2nd edition

Kieran Healy, Duke University, kieran.healy@duke.edu

https://r-graphics.org/

Welcome to the R Graphics Cookbook, a practical guide that provides more than 150 recipes to help you generate high-quality graphs quickly, without having to comb through all the details of R’s graphing systems. Each recipe tackles a specific problem with a solution you can apply to your own project, and includes a discussion of how and why the recipe works.

### Data Visualization. A practical introduction

https://socviz.co/

My main goal is to introduce you to both the ideas and the methods of data visualization in a sensible, comprehensible, reproducible way. Some classic works on visualizing data, such as The Visual Display of Quantitative Information (Tufte, 1983), present numerous examples of good and bad work together with some general taste-based rules of thumb for constructing and assessing graphs. In what has now become a large and thriving field of research, more recent work provides excellent discussions of the cognitive underpinnings of successful and unsuccessful graphics, again providing many compelling and illuminating examples (Ware, 2008).

### Fundamentals of Data Visualization

Claus O. Wilke

The book is meant as a guide to making visualizations that accurately reflect the data, tell a story, and look professional. It has grown out of my experience of working with students and postdocs in my laboratory on thousands of data visualizations. Over the years, I have noticed that the same issues arise over and over. I have attempted to collect my accumulated knowledge from these interactions in the form of this book.

The entire book is written in R Markdown, using RStudio as my text editor and the bookdown package to turn a collection of markdown documents into a coherent whole. The book’s source code is hosted on GitHub, at https://github.com/clauswilke/dataviz. If you notice typos or other issues, feel free to open an issue on GitHub or submit a pull request. If you do the latter, in your commit message, please add the sentence “I assign the copyright of this contribution to Claus O. Wilke,” so that I can maintain the option of publishing this book in other forms.

## Cărți data science terminal

### Data Science at the Command Line, 2e

Jeroen Janssens, June 29, 2021

https://www.datascienceatthecommandline.com/2e/

This book is about doing data science at the command line. My aim is to make you a more efficient and productive data scientist by teaching you how to leverage the power of the command line.

Having both the terms data science and command line in the title requires an explanation. How can a technology that is over 50 years old1 be of any use to a field that is only a few years young?

Today, data scientists can choose from an overwhelming collection of exciting technologies and programming languages. Python, R, Julia, and Apache Spark are but a few examples. You may already have experience in one or more of these. If so, then why should you still care about the command line for doing data science? What does the command line have to offer that these other technologies and programming languages do not?

## Reviste

## Journal of Statistical Software

https://www.jstatsoft.org/index

## Articole

### Tidy Data

https://www.jstatsoft.org/article/view/v059i10

## Discipline emergente

### Computational literacy

#### Computational literacy for the humanities and social science

 https://jiemakel.gitbook.io/clit4hss/

The student understands the multiple ways in which methods benefit work within the computational human sciences. 
She herself is able to use ready tools to work with data. 
In addition, she has attained knowledge of the fundamental concepts of programming, through which she can start to expand her capabilities, should she so choose. 
She also learns how open, reproducible research and publishing is done in practice. 
Further, the student gains a general literacy on advanced statistical and computer science methods applicable to computational human sciences, and when to apply them (as well as crucially, when and how not to apply them). 
Finally, she learns to apply all of the above in practice in a small concrete computational human sciences project. 

#### Minimal computing

We use “minimal computing” to refer to computing done under some set of significant constraints of hardware, software, education, network capacity, power, or other factors. Minimal computing includes both the maintenance, refurbishing, and use of machines to do DH work out of necessity along with the use of new streamlined computing hardware like the Raspberry Pi or the Arduino micro controller to do DH work by choice. This dichotomy of choice vs. necessity focuses attention on computing that is decidedly not high-performance. By operating at this intersection between choice and necessity minimal computing forces important concepts and practices within the DH community to the fore. In this way minimal computing is also an critical movement, akin to environmentalism, asking for balance between gains and costs in related areas that include social justice issues and de-manufacturing and reuse, not to mention re-thinking high-income assumptions about “e-waste” and what people do with it. Minimal computing thus relates to issues of aesthetics, culture, environment, global relationships of power and knowledge production, and other economic, infrastructural and material conditions.

https://go-dh.github.io/mincomp/about/

## Curricule

### Digital Humanities

#### HELSINKI CENTRE FOR DIGITAL HUMANITIES

https://www2.helsinki.fi/en/helsinki-centre-for-digital-humanities/teaching

HELDIG offers to the University a tiered teaching curriculum that goes ever deeper into Digital Humanities, from a single introductory bachelor's course through differently sized minor study blocks to a full-blown master's track.

These different levels of engagement are further described below in size order. In addition to these, HELDIG also runs a Digital Humanities Research Seminar targeted mainly at doctoral students and researchers.

### Universitatea din București

#### Master Digital Humanities

Prezentare program

Programul de masterat “Digital Humanities”
Facultatea de Limbi și Literaturi Străine, Universitatea din București, 2020-2022

Programul de masterat Digital Humanities intră în al doilea an de existență. Primul an s-a dovedit a fi extrem de productiv, atât pentru studenți, cât și pentru profesori, programul câștigând Premiul Universității din București pentru cel mai inovator program universitar/post-universitar. Un scurt interviu despre funcționarea programului în primul său an aici: https://unibuc.ro/fiecare-are-o-motivatie-personala-dar-ceea-ce-cred-ca-au-in-comun-studentii-este-o-anumita-deschidere-flexibilitate-si-dorinta-de-a-invata-ceva-nou-diferit-de-ceea-ce-au-acumulat-pana-in-p/

**Motivația și obiectivul programului masteral Digital Humanities**

Programul de masterat profesional Digital Humanities în limba engleză, ZI este primul masterat din Romanânia care vine în întâmpinarea nevoii formării de specialişti în domeniul Digital Humanities, încadrându-se într-un curent inovator de integrare a două domenii ştiinţifice:

1. Tehnologiile digitale (metode de reprezentare, editare, procesare, vizualizare și interpretare) care asigură ştiinţelor umaniste rigoarea și baza formală pentru o modernizare atât de necesară;
2. Domeniul umanist (lingvistică, literatură, filologie clasică, istorie, sociologie, patrimoniu cultural, etc.), care reprezintă o provocare pentru domeniul IT, prin complexitatea si cantitatea datelor generate.    

**Obiectivul principal** al acestui program masteral este formarea de absolvenţi capabili să stapânească tehnologia digitală şi să o aplice la probleme practice din domeniul umanist. Ca atare, programul se adresează atât absolvenților de facultăți umaniste (limbi străine, litere, științe sociale, istorie, etc.), cât și absolvenților de facultăți cu profil tehnic (informatica, automatica, matematica, etc.).

**Utilitatea pentru pregatirea de specialişti apţi să răspundă cerinţelor de pe piața muncii**

Recent, pe piaţa muncii sunt din ce în ce mai cautaţi specialişti în domenii interdisciplinare, ca procesarea limbajului natural, analişti si programatori cu o pregătire solidă în domeniul lingvisticii, literaturii, socio-lingvisticii, istoriei, cât şi lingvişti şi umanişti în general cu pregătire temeinică în aplicaţii si tehnologii IT. Datorită exploziei volumului și complexității datelor digitale din domeniul umanist din ultimul deceniu, nevoia de specialiști care să le editeze, adnoteze, proceseze, analizeze şi interpreteze prin tehnici şi unelte informatice specifice, a devenit incontestabilă. Stocarea şi prelucrarea manuală a devenit imposibilă, capacitatea de memorare şi procesare a calculatorului fiind prin urmare indispensabilă.

Editarea digitală de text și conservarea patrimoniului cultural sunt unele dintre temele majore în domeniu şi unele dintre ocupaţiile cele mai promiţătoare pentru viitorii absolvenţi.
Indiferent de locul de muncă vizat de absolvenţii programului propus, competenţele digitale reprezintă o calificare necesară şi extrem de actuală pe piaţa muncii. Avantajul instruirii formale IT este de necontestat astăzi pe piaţa muncii.

**Planul de învățământ**

Planul este structurat pe 2 ani, 4 semestre, cu un număr de 14 ore/săptămână și un număr total de 123 de credite.

Cursurile se vor ține în cadrul sediului Facultății de Limbi și Literaturi Străine din strada Pitar Moș, nr. 7-13.

Disciplinele sunt repartizate în mod echilibrat pe cele 4 semestre și organizate în 6 module, care se extind pe parcursul celor doi ani astfel:

##### Reprezentarea digitală a obiectelor:
- Introducere în digitalizarea în științele umaniste
- Metode și aplicații în lingvistica de corpus
- Programare de bază pentru științele umaniste
- Reprezentarea cunoștințelor și ontologii

##### Metode digitale aplicate în patrimoniul cultural:
- Metode și unelte pentru editarea textului
- Patrimoniul cultural digital în Balcani
- Biblioteconomie și organizarea informației

##### Lingvistică:
- Lingvistică generală
- Dimensiuni ale variației lingvistice
- Structura limbii
- Semantică lexicală

##### Informatică pentru științele umaniste:
- Reprezentări digitale ale codificării de text
- Baze de date pentru științele umaniste
- Statistică și probabilități pentru științele umaniste: metodologia experimentelor-lucrul în R
- Introducere în metode de învățare automata: unelte și algoritmi

##### Procesare de text:
- Analiză lingvistică de text
- Fundamentele procesării limbajului natural și aplicații
- Semantică formală și distribuțională
- Topici speciale

##### Opționale:
- Opțional 1 (Analiză cognitivă și social media / Metode de vizualizare pentru științele umaniste)
- Opțional 2 (Greacă veche: cuvinte, concepte, istorii / O perspectivă tipologică asupra partitivității)
- Opțional 3 (Lexicologie biblică latină / Limbile moderne din perspectiva Indo-europeană)

##### Înscrieri admitere sesiunea iulie 2020

Înscrierile se vor face la secretariatul Facultății de Limbi și Literaturi Străine din strada Edgar Quinet nr. 5-7, în perioada 10-20 iulie.

**Data susținerii examenului de admitere**: 23 iulie 2020

**Modalitatea de admitere:**

- dosar (CV, rezumatul lucrării de licență – max 2 pagini, scrisoare de motivație, certificat de competență lingvistică pentru limba engleză eliberat de catre instituțiile abilitate
- interviu online (candidații vor primi pe e-mail linkul pentru participarea la conferința online și ora la care vor intra)

**Condiții de înscriere**: diplomă de bacalaureat și diplomă de studii universitare ciclul I.

**Responsabil** program masteral Digital Humanities: Lector dr. Anca Dinu
Adresă e-mail: [ancaddinu@gmail.com](mailto:ancaddinu@gmail.com) Tel.: 0785641041

http://admitere.unibuc.ro/noul-program-de-masterat-digital-humanities-la-facultatea-de-limbi-si-literaturi-straine/

https://unibuc.ro/fiecare-are-o-motivatie-personala-dar-ceea-ce-cred-ca-au-in-comun-studentii-este-o-anumita-deschidere-flexibilitate-si-dorinta-de-a-invata-ceva-nou-diferit-de-ceea-ce-au-acumulat-pana-in-p/

https://romanice.lls.unibuc.ro/programe-de-studii/masterat/

## Arhive web

http://data.webarchive.org.uk/opendata/

## Workshopuri

### ggplot2 workshop, Thomas Lin Pedersen

Include și gganimate

part 1: https://www.youtube.com/watch?v=h29g21z0a68

part 2: https://www.youtube.com/watch?v=0m4yywqNPVY

https://github.com/thomasp85/ggplot2_workshop.git

### The Programming Historian

https://programminghistorian.org/

https://programminghistorian.org/en/lessons/cleaning-data-with-openrefine

## Repo-uri Github

### rfordatasciece

https://github.com/rfordatascience

Are un repo interesant https://github.com/rfordatascience/tidytuesday

Join the `R4DS Online Learning Community` in the weekly `#TidyTuesday` event! Every week we post a raw dataset, a chart or article related to that dataset, and ask you to explore the data. While the dataset will be “tamed”, it will not always be tidy! As such you might need to apply various `R for Data Science` techniques to wrangle the data into a true tidy format. The goal of `TidyTuesday` is to apply your R skills, get feedback, explore other’s work, and connect with the greater `#RStats` community! As such we encourage everyone of all skills to participate!

We will have many sources of data and want to emphasize that **no causation** is implied. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our guidelines are to use the data provided to practice your data tidying and plotting techniques. Participants are invited to consider for themselves what nuancing factors might underlie these relationships.

The intent of Tidy Tuesday is to provide a safe and supportive forum for individuals to practice their **wrangling** and **data visualization** skills independent of drawing conclusions. While we understand that the two are related, the focus of this practice is purely on building skills with real-world data.

A weekly data project aimed at the R ecosystem. As this project was borne out of the `R4DS Online Learning Community` and the `R for Data Science` textbook, an emphasis was placed on understanding how to summarize and arrange data to make meaningful charts with `ggplot2`, `tidyr`, `dplyr`, and other tools in the `tidyverse` ecosystem. However, any code-based methodology is welcome - just please remember to share the code used to generate the results.

### JSON-Splora

https://github.com/wellsjo/JSON-Splora

JSON-Splora is a GUI for editing, visualizing, and manipulating JSON data with jq or JavaScript.

### node-jq

https://github.com/sanack/node-jq

node-jq is a Node.js wrapper for jq - a lightweight and flexible command-line JSON processor

## Cum să scrii materialele de curs

https://programminghistorian.org/en/author-guidelines

## Formate de date

### JSON5

https://json5.org/

The JSON5 Data Interchange Format (JSON5) is a superset of JSON that aims to alleviate some of the limitations of JSON by expanding its syntax to include some productions from ECMAScript 5.1.

This JavaScript library is the official reference implementation for JSON5 parsing and serialization libraries.

## Blockchain

### Blockchain - Revealing the Myth

https://open.hpi.de/courses/blockchain2021

## Omeka

### Docker
https://hub.docker.com/r/libnamic/omeka-s

## Linked Data

### Linked Data Engineering

https://www.youtube.com/watch?v=YZKyT3RYWkc

## Semantic Web

### Semantic Web Technologies -serie

https://www.youtube.com/watch?v=KBf5YM_OhIM&list=PLoOmvuyo5UAeihlKcWpzVzB51rr014TwD&index=145

### SPARQL

http://www.learningsparql.com/

## Artificial Intelligence

### Netron

https://github.com/lutzroeder/netron

# Data visualisation

## Gephi

https://gephi.org/

https://seinecle.github.io/gephi-tutorials/

https://firstdraftnews.org/long-form-article/trackers-gephi-dmi/

## JSON visualisations

https://jsoncrack.com/

## RAWGraphs

https://www.rawgraphs.io/

## Datawrapper

## sigmajs.org

https://www.sigmajs.org/

a JavaScript library aimed at visualizing graphs of thousands of nodes and edges

## Data visualisation

Este o carte pusă spre consultare liberă.
https://clauswilke.com/dataviz/

O vizualizare superbă de la Wikidata: https://www.youtube.com/redirect?event=comments&redir_token=QUFFLUhqbUNiRExwZDJSZHMxNkZvT2l3SjU4X09WX1g0QXxBQ3Jtc0trWFpNX200MHdkelI5YTl4emJOcHJXQ2p3U1F0Y3lGUXZ6bkxEZzVUWS1mOEZ4UUt6cjlHTzBUMXdIZXhIcWhxN3AwVzE2X3JKekh3VFdWbXQzWF9XT3ZuSkoyUTdJT09OZ0trTEFra00yVE1nZWNMcw&q=https%3A%2F%2Fw.wiki%2FBUA&stzid=UgwH_lbQr9tUYZiMLSt4AaABAg

## Hands-On Data Visualization

https://handsondataviz.org/

## Wikidata

Wikidata:Linked open data workflow
https://www.wikidata.org/wiki/Wikidata:Linked_open_data_workflow

# Reprezentarea cunoașterii

## [Angus Addlesee](http://addlesee.co.uk/)

1. [Understanding Linked Data Formats: Turtle vs RDF/XML vs N-Triples vs JSON-LD | Angus Addlesee](https://medium.com/wallscope/understanding-linked-data-formats-rdf-xml-vs-turtle-vs-n-triples-eb931dbe9827)
2. [Creating Linked Data | Angus Addlesee](https://medium.com/wallscope/creating-linked-data-31c7dd479a9e)
3. [Linked Data Reconciliation in GraphDB: Using DBpedia to Enhance your Data in GraphDB | Angus Addlesee](https://medium.com/wallscope/linked-data-reconciliation-in-graphdb-cd2796d2870b)

## EUCLID - EdUcational Curriculum for the usage of LInked Data

https://euclid-project.eu/index.html
https://euclid-project.eu/modules/chapter4.html

## OntoRefine

https://disc-semantic.uibk.ac.at/ontorefine

OntoRefine is an extension of OpenRefine that adds support for direct RDF conversion through a SPARQL endpoint. The current OntoRefine is based on OpenRefine v3.3.

https://medium.com/wallscope/using-ontorefine-to-transform-tabular-data-into-linked-data-7277ec8c2c0f

## OpenRefine

https://openrefine.org/download.html

https://mdl.library.utoronto.ca/technique/cleaning-data
https://mdl.library.utoronto.ca/technology/tutorials/openrefine-augmenting-activity-1-preparing-data

Introduction to OpenRefine day 1 by Sandra Fauconnier
https://www.youtube.com/watch?v=9mdHlSx_iXM

Introduction to OpenRefine day 2 by Sandra Fauconnier
https://www.youtube.com/watch?v=WYS8fTjFVJ8

## Documente vii

https://paperswithcode.com/

### Observable notebooks

### Obsidian.md

# Cum să instalezi

https://www.youtube.com/watch?v=DMOWhqp6lHQ

First contact with Gephi 0.9.2

https://www.youtube.com/watch?v=YM_37z_uURM

Layouts, clusters, and where to find them

https://www.youtube.com/watch?v=0LqY8OfSsKE

## Machine learning

https://datamachines.xyz/the-hands-on-reinforcement-learning-course-page/

## Python

https://www.youtube.com/user/Albert10110
